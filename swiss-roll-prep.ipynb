{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec46280",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T14:16:53.313231Z",
     "iopub.status.busy": "2025-06-25T14:16:53.312897Z",
     "iopub.status.idle": "2025-06-25T14:17:10.536042Z",
     "shell.execute_reply": "2025-06-25T14:17:10.534842Z"
    },
    "papermill": {
     "duration": 17.229402,
     "end_time": "2025-06-25T14:17:10.537970",
     "exception": false,
     "start_time": "2025-06-25T14:16:53.308568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing packages: torch, numpy torchvision etc. If this is the first run this could take about a minute.\n",
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2024 authors of the paper \"Generative Topological Networks\".\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import torch\n",
    "\n",
    "random_seed = 8745\n",
    "alpha = 0.3  # for scatter plots\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set the value of d (latent dimension) for both the autoencoder and gtn:\n",
    "d_mnist = 5\n",
    "d_celeba = 100\n",
    "d_hap = 50\n",
    "\n",
    "print(\"Importing packages: torch, numpy torchvision etc. If this is the first run this could take about a minute.\")\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import pickle as pkl\n",
    "import torchvision.utils\n",
    "from PIL import Image\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "print(\"Imports complete.\")\n",
    "\n",
    "\n",
    "phi, phi_inv = norm.cdf, norm.ppf\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def make_dirs(dirs):\n",
    "    for dir in dirs:\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)\n",
    "\n",
    "\n",
    "def savePkl(data, dir, filename):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "    handler = open(dir+filename, 'wb')\n",
    "    pkl.dump(data, handler)\n",
    "    handler.close()\n",
    "\n",
    "\n",
    "def loadPkl(dir, filename):\n",
    "    handler = open(dir+filename, 'rb')\n",
    "    data = pkl.load(handler)\n",
    "    handler.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def swiss_roll(theta):\n",
    "    return theta * [math.cos(i) for i in theta], theta * [math.sin(i) for i in theta]\n",
    "\n",
    "\n",
    "def save_dataset_as_torch(train, val, test, dir_dataset, scale_01=False):\n",
    "    if scale_01:\n",
    "        train_mean, train_std = torch.mean(train, axis=0), torch.std(train, axis=0)\n",
    "    else:\n",
    "        train_mean, train_std = 0, 1\n",
    "\n",
    "    # Normalize using train stats\n",
    "    train = (train - train_mean) / train_std\n",
    "    val = (val - train_mean) / train_std\n",
    "    test = (test - train_mean) / train_std\n",
    "\n",
    "    train = train.type(torch.float32)\n",
    "    val = val.type(torch.float32)\n",
    "    test = test.type(torch.float32)\n",
    "\n",
    "    if not os.path.exists(dir_dataset):\n",
    "        os.makedirs(dir_dataset)\n",
    "\n",
    "    torch.save(train, dir_dataset + 'train.pt')\n",
    "    torch.save(val, dir_dataset + 'val.pt')\n",
    "    torch.save(test, dir_dataset + 'test.pt')\n",
    "\n",
    "    torch.save(train_mean, dir_dataset+'train_mean.pt')\n",
    "    torch.save(train_std, dir_dataset+'train_std.pt')\n",
    "\n",
    "\n",
    "def calc_normalization_stats_from_train(path_train, dir_dataset):\n",
    "\n",
    "    data_train = torch.load(path_train, map_location=torch.device(DEVICE))\n",
    "\n",
    "    train_mean, train_std = torch.mean(data_train, dim=0), torch.std(data_train, dim=0)\n",
    "\n",
    "    torch.save(train_mean, dir_dataset+'train_mean.pt')\n",
    "    torch.save(train_std, dir_dataset+'train_std.pt')\n",
    "\n",
    "    return train_mean, train_std\n",
    "\n",
    "\n",
    "def create_single_image_from_tensor(img_tensor, reverse_normalization=True):\n",
    "    if reverse_normalization:\n",
    "        img_tensor = 0.5 * img_tensor + 0.5\n",
    "        img_tensor = img_tensor.clamp(0, 1)\n",
    "\n",
    "    img_tensor = img_tensor.numpy()\n",
    "    img_tensor = np.transpose(img_tensor, (1, 2, 0))\n",
    "    return img_tensor\n",
    "\n",
    "\n",
    "def save_images_grid(data, out_folder, filename, save=False, reverse_normalization=True, nrow=10, dpi=2000):\n",
    "    print(\"Plotting using dpi={}. This may be slow for high dpi and may take up extra space.\".format(dpi))\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    img = create_single_image_from_tensor(torchvision.utils.make_grid(data, nrow, 5), reverse_normalization)\n",
    "    plt.imshow(img)\n",
    "    plt.tick_params(\n",
    "        axis='x',\n",
    "        which='both',\n",
    "        bottom=False,\n",
    "        top=False,\n",
    "        labelbottom=False)\n",
    "    plt.tick_params(\n",
    "        axis='y',\n",
    "        which='both',\n",
    "        left=False,\n",
    "        right=False,\n",
    "        labelleft=False,\n",
    "        labelright=False)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(out_folder + filename, dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def calc_normalization_stats_from_train(path_train, dir_dataset):\n",
    "\n",
    "    data_train = torch.load(path_train, map_location=torch.device(DEVICE))\n",
    "\n",
    "    train_mean, train_std = torch.mean(data_train, dim=0), torch.std(data_train, dim=0)\n",
    "\n",
    "    torch.save(train_mean, dir_dataset+'train_mean.pt')\n",
    "    torch.save(train_std, dir_dataset+'train_std.pt')\n",
    "\n",
    "    return train_mean, train_std\n",
    "\n",
    "\n",
    "class NormalToTrgtDataset(Dataset):\n",
    "    '''\n",
    "    Creates a dataset object that produces (y, x_y) pairs where y is sampled from the standard normal distribution and\n",
    "    x_y is from the dataset (see paper).  It is advisable to have the data samples x_y roughly 0-centred. Note that below,\n",
    "    x and y represent input and labels respectively, for consistency with typical training notation (so x below is\n",
    "    actually the y from the pair (y, x_y) and y below is actually the x_y from the pair (y, x_y).\n",
    "    '''\n",
    "    def __init__(self, trgt_filepath, dataset_path, subset, transform=None, target_transform=None, type=\"cosine\", n_samples_max=10 ** 8,\n",
    "                 train_mean=None, train_std=None, n_clusters=1, noise_target_by=0,\n",
    "                 kmeans=None, cluster_to_mean=None, cluster_to_std=None):\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.trgt = torch.load(trgt_filepath, map_location=torch.device(DEVICE))[:n_samples_max]\n",
    "        self.cluster_to_target_norms = {}\n",
    "        self.d = self.trgt.shape[-1]\n",
    "        if noise_target_by > 0:\n",
    "            self.trgt = torch.normal(self.trgt, noise_target_by) #0.0001\n",
    "\n",
    "        clusters, kmeans = make_clusters(x=self.trgt, n_clusters=n_clusters, kmeans=kmeans)\n",
    "        self.kmeans = kmeans\n",
    "\n",
    "        (self.x, self.y, self.cluster_to_mean, self.cluster_to_std,\n",
    "         self.cluster_to_rays, self.cluster_to_sampling_weight) \\\n",
    "            = combine_labels_from_clusters(clusters, cluster_to_mean, cluster_to_std, subset, self.trgt.shape[0])\n",
    "        torch.save(self.x, dataset_path + 'x_{}_{}_n_clusters_{}.pt'.format(subset, n_samples_max, n_clusters))\n",
    "        torch.save(self.y, dataset_path + 'y_{}_{}_n_clusters_{}.pt'.format(subset, n_samples_max, n_clusters))\n",
    "\n",
    "        torch.save(self.cluster_to_mean, dataset_path + '{}_cluster_to_mean_n_clusters_{}.pt'.format(subset, n_clusters))\n",
    "        torch.save(self.cluster_to_std, dataset_path + '{}_cluster_to_std_n_clusters_{}.pt'.format(subset, n_clusters))\n",
    "        torch.save(self.cluster_to_rays, dataset_path + '{}_cluster_to_rays_n_clusters_{}.pt'.format(subset, n_clusters))\n",
    "        torch.save(self.cluster_to_sampling_weight, dataset_path + '{}_cluster_to_sampling_weight_n_clusters_{}.pt'.format(subset, n_clusters))\n",
    "        torch.save(self.cluster_to_target_norms, dataset_path + '{}_cluster_to_target_norms_n_clusters_{}.pt'.format(subset, n_clusters))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "        return x, y\n",
    "\n",
    "def make_clusters(x, n_clusters, kmeans):\n",
    "    # from torch_kmeans import KMeans\n",
    "    from fast_pytorch_kmeans import KMeans\n",
    "\n",
    "    if kmeans is not None:\n",
    "        labels = kmeans.predict(x)\n",
    "    else:\n",
    "        # model = KMeans(n_clusters=n_clusters)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, mode='euclidean', verbose=0)\n",
    "        labels = kmeans.fit_predict(x)\n",
    "\n",
    "    clusters = {}\n",
    "    for i in range(n_clusters):\n",
    "        sample_idx_for_cluster = torch.where(labels == i)[-1]\n",
    "        samples_for_cluster = x[sample_idx_for_cluster]\n",
    "        clusters[i] = samples_for_cluster\n",
    "    print(\"Done clusters\")\n",
    "\n",
    "    # identify singleton or empty clusters and remove them\n",
    "    singleton_cluster_ids = []\n",
    "    for c in clusters.keys():\n",
    "        if len(clusters[c]) <= 1:\n",
    "            singleton_cluster_ids.append(c)\n",
    "    for c in singleton_cluster_ids:\n",
    "        del clusters[c]\n",
    "    print(\"Removed {} singleton or empty clusters.\".format(len(singleton_cluster_ids)))\n",
    "\n",
    "    return clusters, kmeans\n",
    "\n",
    "\n",
    "def combine_labels_from_clusters(clusters, cluster_to_mean, cluster_to_std, subset, n_data):\n",
    "    target = None\n",
    "    if subset == 'train':\n",
    "        cluster_ids = clusters.keys()\n",
    "    else:\n",
    "        cluster_ids = cluster_to_mean.keys()\n",
    "    cluster_to_rays, cluster_to_target_norms, cluster_to_sampling_weight = {}, {}, {}\n",
    "    if subset == 'train':\n",
    "        cluster_to_mean, cluster_to_std = {}, {}\n",
    "    for i in cluster_ids:\n",
    "        if i not in clusters.keys():\n",
    "            continue\n",
    "        target_cluster = torch.tensor(clusters[i], device=DEVICE)\n",
    "        if subset == 'train':\n",
    "            cluster_to_sampling_weight[i] = target_cluster.shape[0] / n_data\n",
    "        if subset == 'train':\n",
    "            target_mean_cluster, target_std_cluster = torch.mean(target_cluster, dim=0), torch.std(target_cluster, dim=0)\n",
    "        else:\n",
    "            target_mean_cluster, target_std_cluster = cluster_to_mean[i], cluster_to_std[i]\n",
    "\n",
    "        # normalize for within-cluster labeling with standard normal source\n",
    "        target_cluster = (target_cluster - target_mean_cluster) / target_std_cluster\n",
    "        s = torch.randn(target_cluster.shape, device=DEVICE)\n",
    "        source_cluster, target_cluster = create_labels(s, target_cluster)\n",
    "\n",
    "        # remove normalization from target and do the same for source:\n",
    "        target_cluster = target_cluster * target_std_cluster + target_mean_cluster\n",
    "        source_cluster = source_cluster * target_std_cluster + target_mean_cluster\n",
    "\n",
    "        if target is None:\n",
    "            target = target_cluster\n",
    "            source = source_cluster\n",
    "        else:\n",
    "            target = torch.cat([target, target_cluster], dim=0)\n",
    "            source = torch.cat([source, source_cluster], dim=0)\n",
    "        cluster_to_mean[i], cluster_to_std[i] = target_mean_cluster, target_std_cluster\n",
    "    # shuffling the training data\n",
    "    permuted_idx = np.random.permutation(range(len(target)))\n",
    "    target = target[permuted_idx]\n",
    "    source = source[permuted_idx]\n",
    "\n",
    "    return source, target, cluster_to_mean, cluster_to_std, cluster_to_rays, cluster_to_sampling_weight  # concatenates from all clusters source_cluster and target_cluster and randomly permutes them too\n",
    "\n",
    "\n",
    "class HandsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "        df = pd.read_csv(csv_path, index_col=None)\n",
    "        self.img_dir = img_dir\n",
    "        self.csv_path = csv_path\n",
    "        self.img_names = df['Filename']\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.img_dir,\n",
    "                                      str(self.img_names[index])))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor([0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "\n",
    "class EncodedImagesDataset(Dataset):\n",
    "    def __init__(self, trgt_filepath, transform=None, target_transform=None):\n",
    "        self.x = torch.load(trgt_filepath, map_location=torch.device(DEVICE))\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_labels(s, t):\n",
    "    '''\n",
    "    Creates the labeled pairs (y, x_y) as described in the paper.\n",
    "    :param s: tensor of shape n_samples  x  d (d = latent dimension, see paper)\n",
    "                representing source distribution (Y in paper) to map to target t (next param)\n",
    "    :param t: tensor of shape n_samples  x  d (d = latent dimension, see paper) representing target (X in paper)\n",
    "    :return: source s with matching labels from target\n",
    "    '''\n",
    "    print(\"Creating labeled data.\")\n",
    "\n",
    "    t_norm, s_norm = torch.linalg.norm(t,dim=1), torch.linalg.norm(s, dim=1)\n",
    "    # sort t and s by norm\n",
    "    t_norm_sorted, t_norm_sort_indices = torch.sort(t_norm, dim=0)\n",
    "    _, s_norm_sort_indices = torch.sort(s_norm, dim=0)\n",
    "    s, t = s[s_norm_sort_indices], t[t_norm_sort_indices]\n",
    "\n",
    "    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    labels = torch.zeros(t.shape, device=DEVICE)\n",
    "\n",
    "    print(\"\\nComputing cosine similarities.\")\n",
    "\n",
    "    for i in tqdm(range(s.shape[0])):\n",
    "        s_i = s[i]\n",
    "\n",
    "        # max cosine similarity\n",
    "        #-----------------------\n",
    "        res = cos(s_i, t)\n",
    "        max_ix = torch.argmax(res)\n",
    "        # max_ix = 0\n",
    "        t_closest_cos_sim = t[max_ix]\n",
    "        labels[i] = t_closest_cos_sim\n",
    "        t = torch.cat((t[0:max_ix], t[max_ix + 1:]))  # what remains\n",
    "\n",
    "    labels = labels.type(torch.float32)\n",
    "\n",
    "    return s, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d63478",
   "metadata": {
    "_cell_guid": "001e3216-51cd-465b-b53c-85ea34694c91",
    "_uuid": "2c118fde-26ba-4010-84e0-57fea7bd595e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-25T14:17:10.548766Z",
     "iopub.status.busy": "2025-06-25T14:17:10.547699Z",
     "iopub.status.idle": "2025-06-25T14:17:10.772603Z",
     "shell.execute_reply": "2025-06-25T14:17:10.771454Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.231423,
     "end_time": "2025-06-25T14:17:10.774175",
     "exception": false,
     "start_time": "2025-06-25T14:17:10.542752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the data.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2024 authors of the paper \"Generative Topological Networks\".\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "def make_swiss_roll_dataset(dir_dataset, n_samples):\n",
    "\n",
    "    make_dirs([dir_dataset])\n",
    "\n",
    "    # train set\n",
    "    a = np.random.uniform(1.5*math.pi, 4.5*math.pi, size=n_samples)\n",
    "    a_train = torch.tensor(sorted(a), device=DEVICE)\n",
    "    a_train = torch.unsqueeze(a_train, dim=1)\n",
    "\n",
    "    # val set\n",
    "    a = np.random.uniform(1.5*math.pi, 4.5*math.pi, size=n_samples//5)  # to mimic the usual 20% val, test\n",
    "    a_val = torch.tensor(sorted(a))\n",
    "    a_val = torch.unsqueeze(a_val, dim=1)\n",
    "\n",
    "    # test set\n",
    "    a = np.random.uniform(1.5*math.pi, 4.5*math.pi, size=n_samples//5)  # to mimic the usual 20% val, test\n",
    "    a_test = torch.tensor(sorted(a))\n",
    "    a_test = torch.unsqueeze(a_test, dim=1)\n",
    "\n",
    "    save_dataset_as_torch(a_train, a_val, a_test, dir_dataset, scale_01=True)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Preparing the data.\")\n",
    "    make_swiss_roll_dataset('../data/swiss_roll/', n_samples=100000)\n",
    "    print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.205076,
   "end_time": "2025-06-25T14:17:14.345890",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-25T14:16:48.140814",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
